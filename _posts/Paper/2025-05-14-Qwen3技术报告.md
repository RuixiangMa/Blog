---
title: Qwen3 技术报告
categories: [Paper]
tags: Qwen3 Inference  LLM 
---

# Qwen3 技术报告

## 一、训练流程

Qwen3 的训练分为两个主要阶段：**预训练（Pre-training）** 和 **后训练（Post-training）**。

整个训练流程包括 **四个阶段**，其中前两个阶段专注于“思考”能力的培养，后两个阶段则强化“非思考”能力。

---

### 1. 预训练（Pre-training）

#### ✅ 训练数据规模
- 使用约 **36 万亿 token** 的大规模语料库。
- 数据来源包括：
  - 多模态文本提取（如 PDF 文档）
  - 合成数据生成（使用 Qwen2.5-Math 和 Qwen2.5-Coder 生成数学和代码内容）

#### ✅ 预训练三阶段策略

| 阶段 | 目标 | 内容 |
|------|------|------|
| Stage 1 (General) | 构建通用知识基础 | 训练超过 30T tokens，学习基本的语言理解能力 |
| Stage 2 (Knowledge-intensive) | 强化逻辑推理能力 | 增强 STEM（科学、技术、工程、数学）和编程能力 |
| Stage 3 (Long-context) | 提升上下文长度 | 将最大上下文长度从 4,096 扩展到 32,768 tokens |

> ⚠️ 第三阶段使用了长上下文训练数据，使模型能处理更复杂的文档摘要、对话历史追踪等任务。

### 2. 后训练（Post-training）

目标是让模型既能进行深度推理（Thinking Mode），也能快速响应（Non-Thinking Mode），并提升其与人类偏好对齐的能力。

<p align="center">
  <img src="https://github.com/RuixiangMa/lancer.github.io/blob/main/_posts/Paper/image-14.png?raw=true" alt="Paper Image" width="300">
</p>

#### 四个后训练阶段

| 阶段 | 目标 | 方法 |
|------|------|------|
| Stage 1 | 思维冷启动 | 使用 Long Chain-of-Thought（CoT）微调 |
| Stage 2 | 强化学习（RL） | 在数学和编程任务上应用 RL，提升推理能力 |
| Stage 3 | 融合思维与非思维数据 | 混合 CoT 与非 CoT 数据，统一训练 |
| Stage 4 | 全领域强化学习 | 提升模型在各类下游任务中的表现 |

Stage 3 中的数据融合采用了基于实例级别的混合策略（instance-level data mixing），通过在代理模型上进行大量消融实验来优化数据分布。

## 二、模型架构设计

Qwen3 系列包含多个模型版本，覆盖从轻量级到超大规模场景。

<p align="center">
  <img src="https://github.com/RuixiangMa/lancer.github.io/blob/main/_posts/Paper/image-15.png?raw=true" alt="Paper Image" width="300">
</p>


## 三、推理模式控制机制（Thinking/Non-Thinking Mode）

Qwen3 支持两种推理模式：

### 1. Thinking Mode（思考模式）
- 用于复杂任务如逻辑推理、数学计算、编程等。
- 使用 CoT 进行深度思考。
- 可通过 `/think` 指令手动启用。

### 2. Non-Thinking Mode（非思考模式）
- 用于快速响应简单指令、日常对话等。
- 不使用 CoT，直接输出简洁结果。
- 默认模式，可通过 `/no think` 显式指定。

📊 实验表明，在 128K 上下文长度下，Thinking 模式的接受率略低于 Non-Thinking 模式，但仍在合理范围。

## 四、小模型蒸馏策略（Strong-to-Weak Distillation）

为了提升小模型的性能，Qwen3 采用了一种高效的蒸馏策略：

### 📉 方法概述
- 利用大模型（Teacher Model）的知识指导小模型（Student Model）训练。
  - **Off-policy 蒸馏**：利用 Teacher 模型的历史输出作为监督信号。
  - **On-policy 蒸馏**：结合 Reinforcement Learning 进行动态调整。

### 📈 效果对比

| 指标        | 四阶段训练 | 蒸馏方法       |
|-------------|------------|----------------|
| Pass@1      | 基准       | 提升明显       |
| Pass@64     | 基准       | 探索能力增强   |
| GPU 时间     | 高         | 减少至 1/10    |

✅ 蒸馏方法显著优于四阶段独立训练，尤其在训练效率和探索能力上。

## 五、训练效率优化手段（RL + Entropy Control）

### 🔄 强化学习（Reinforcement Learning）
- 使用 RL 来提升模型的长期奖励。
- 在 AIME’24 数学竞赛任务中，Qwen3-235B-A22B 的得分从 70.1 提高到 85.1，共经历 170 步训练。

### 🔁 控制探索与利用的平衡
- 通过控制模型熵（Entropy）来调节探索与利用的比率。
- 实现自动调整，无需人工干预超参数。

## 六、实验结果与性能评估

### 🧮 数学与推理能力

| 模型                   | MATH-500 | GPQA-Diamond | LiveCodeBench v5 |
|------------------------|----------|---------------|------------------|
| Qwen3-235B-A22B        | 93.4     | 40.1          | 70.7             |
| Qwen3-32B              | 89.1     | 27.9          | 70.6             |
| GPT-4o-2024-1120       | 83.9     | 33.8          | 67.5             |

✅ Qwen3-235B-A22B 表现出接近甚至超越主流闭源模型的数学推理能力。

### 🌐 多语言支持

Qwen3 支持 **119 种语言和方言**，在 Belebele 测试中表现优异：

| 语系             | Qwen3-32B (Thinking) | Qwen3-32B (Non-Thinking) |
|------------------|-----------------------|---------------------------|
| Indo-European    | 90.7                  | 89.1                      |
| Sino-Tibetan     | 89.7                  | 88.0                      |
| Afro-Asiatic     | 84.8                  | 82.3                      |
| Austronesian     | 86.7                  | 83.7                      |

## 总结与研究建议

### ✅ Qwen3 的核心优势
- 双模式推理机制（Thinking/Non-Thinking）
- 多阶段训练流程（4-stage post-training）
- 支持超长上下文（128K token）
- 多语言与跨文化支持
- 高效的小模型蒸馏策略
- 强大的数学与代码生成能力

### 🔬 研究建议
1. **探索性训练策略优化**
   - 结合 RL与熵控制机制，实现自适应探索与利用平衡。

2. **多阶段蒸馏机制改进**
   - 设计更细粒度的 logits distillation 方案，提升小模型性能。

3. **长上下文 KV 缓存管理**
   - 结合 QuantSpec 技术，探索分层量化 KV 缓存以提升推理效率。

4. **多模态下的 Thinking Mode 扩展**
   - 探索视觉、音频输入下的深度推理路径构建。
